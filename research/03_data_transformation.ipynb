{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\MLOPS\\\\ML-Approach-for-Predict-Cancellation-Prevent-Loss-with-MLflow\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\MLOPS\\\\ML-Approach-for-Predict-Cancellation-Prevent-Loss-with-MLflow'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: str\n",
    "    preprocessing_data : str\n",
    "    preprocessor : str\n",
    "    train: Path\n",
    "    test: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.constants import *\n",
    "from src.utils.common import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath = CONFIG_FILE_PATH,      \n",
    "                 schema_filepath = SCHEMA_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir= config.root_dir,\n",
    "            data_path= config.data_path,\n",
    "            preprocessing_data = config.preprocessing_data,\n",
    "            preprocessor = config.preprocessor,\n",
    "            train = config.train,\n",
    "            test = config.test\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from src.utils.logger import logging\n",
    "from src.utils.exception import CustomException\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import  train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataTransformation:\n",
    "    def __init__(self, config=DataTransformationConfig):\n",
    "        self.config = config \n",
    "    \n",
    "    \n",
    "    def save_preprocessing_data(self):\n",
    "        try:\n",
    "            df = pd.read_csv(self.config.data_path)\n",
    "            df[\"booking status\"] = df[\"booking status\"].apply(lambda x : 1 if x=='Not_Canceled' else 0) \n",
    "\n",
    "            df = df[~df[\"date of reservation\"].str.contains(\"-\")]\n",
    "\n",
    "            df[\"date of reservation\"] = pd.to_datetime(df[\"date of reservation\"])\n",
    "            df[\"month\"] = df[\"date of reservation\"].dt.month\n",
    "\n",
    "            df.drop(['Booking_ID','date of reservation'], axis=1, inplace=True)\n",
    "\n",
    "            df.to_csv(self.config.preprocessing_data, index=False, header=True)\n",
    "            logging.info(\"Saving pre processing data\")\n",
    "\n",
    "            return \"Preprocessing data saved successfully\"\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info(\"Error occured in saving preprocess data\")\n",
    "            raise CustomException(e,sys)\n",
    "        \n",
    "\n",
    "\n",
    "    def get_data_transformer_object(self):\n",
    "        try:\n",
    "            self.save_preprocessing_data()\n",
    "\n",
    "            df = pd.read_csv(self.config.preprocessing_data)\n",
    "            X = df.drop('booking status',axis=1)\n",
    "            y = df['booking status']\n",
    "\n",
    "            num_feature = X.select_dtypes(exclude=\"object\").columns \n",
    "            cat_feature = X.select_dtypes(include=\"object\").columns\n",
    "\n",
    "            numeric_transformer = StandardScaler()\n",
    "            oh_transformer = OneHotEncoder()\n",
    "\n",
    "            data_transformer = ColumnTransformer(\n",
    "                [\n",
    "                    (\"OneHotEncoder\", oh_transformer, cat_feature),\n",
    "                    (\"StandardScaler\", numeric_transformer, num_feature),        \n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            preprocessor = Pipeline(steps=[(\"data_transformer\", data_transformer)])\n",
    "\n",
    "            return preprocessor\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info(f\"Error in creating data transformation object: {e}\")\n",
    "            raise CustomException(e,sys)\n",
    "        \n",
    "\n",
    "\n",
    "    def train_test_split(self):\n",
    "        try:\n",
    "            df = pd.read_csv(self.config.preprocessing_data)\n",
    "            logging.info(\"Reading dataset as dataframe\")\n",
    "\n",
    "            logging.info(\"Initiate splitting dataset as train & test set\")\n",
    "            train_set, test_set = train_test_split(df)\n",
    "\n",
    "            train_set.to_csv(\n",
    "                os.path.join(self.config.root_dir,\"train.csv\"), index=False\n",
    "            )\n",
    "\n",
    "            test_set.to_csv(\n",
    "                os.path.join(self.config.root_dir,\"test.csv\"), index=False\n",
    "            )\n",
    "\n",
    "            logging.info(\"Dataset splitted into train and test set\")\n",
    "            logging.info(f\"{train_set.shape}\")\n",
    "            logging.info(f\"{test_set.shape}\")\n",
    "\n",
    "            return (\n",
    "                self.config.train,\n",
    "                self.config.test\n",
    "            )\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.info(\"Error in splitting train and test set\")\n",
    "            raise CustomException(e,sys)\n",
    "\n",
    "\n",
    "    def initiate_data_transformation(self, train_path, test_path):\n",
    "        try:\n",
    "            train_set = pd.read_csv(train_path)\n",
    "            test_set = pd.read_csv(test_path)\n",
    "            logging.info(\"Reading train and test dataset completed\")\n",
    "\n",
    "            logging.info(\"Obtaining preprocessor object\")\n",
    "\n",
    "            preprocessor_obj =  self.get_data_transformer_object() \n",
    "\n",
    "            target_feature = \"booking status\"\n",
    "            \n",
    "            independent_feature = list(['number of adults', 'number of children', 'number of weekend nights',\n",
    "            'number of week nights', 'type of meal', 'car parking space',\n",
    "            'room type', 'lead time', 'market segment type', 'repeated', 'P-C',\n",
    "            'P-not-C', 'average price', 'special requests', 'month'])\n",
    "            \n",
    "            logging.info(\"Dropping target feature from train and test dataframe\")\n",
    "            input_train_df = train_set.drop(columns=[target_feature], axis=1)\n",
    "            target_train_df = train_set[target_feature]\n",
    "\n",
    "            input_test_df = test_set.drop(columns=[target_feature], axis=1)\n",
    "            target_test_df = test_set[target_feature]\n",
    "\n",
    "            logging.info(\"Applying preprocessor object on training and test dataframe\")\n",
    "            \n",
    "            train_df = preprocessor_obj.fit_transform(input_train_df)\n",
    "            test_df = preprocessor_obj.transform(input_test_df)\n",
    "            \n",
    "            logging.info(\"Successfully applied preprocessor object on train and test data\")\n",
    "\n",
    "            joblib.dump(preprocessor_obj, os.path.join(self.config.root_dir, self.config.preprocessor))\n",
    "            logging.info(\"Preprocessor saved to file using joblib\")\n",
    "\n",
    "\n",
    "            train_arr = np.c_[train_df, np.array(target_train_df)]\n",
    "            test_arr = np.c_[test_df, np.array(target_test_df)]\n",
    "            logging.info(\"Returning train and test array\")\n",
    "            \n",
    "            logging.info(\"Saving train and test set in numpy file\")\n",
    "            np.save(os.path.join(self.config.root_dir,\"train_arr.npy\"), train_arr)\n",
    "            np.save(os.path.join(self.config.root_dir,\"test_arr.npy\"), test_arr)\n",
    "\n",
    "            return (\n",
    "                train_arr,\n",
    "                test_arr\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.info(\"Error occured in data transformation\")\n",
    "            raise CustomException(e,sys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2024-04-01 22:13:03,145 ] 31 root - INFO - config\\config.yaml yaml file loaded successfully\n",
      "[ 2024-04-01 22:13:03,158 ] 31 root - INFO - schema.yaml yaml file loaded successfully\n",
      "[ 2024-04-01 22:13:03,175 ] 52 root - INFO - Created directory at : artifacts\n",
      "[ 2024-04-01 22:13:03,182 ] 52 root - INFO - Created directory at : artifacts/data_transformation\n",
      "[ 2024-04-01 22:13:03,855 ] 19 root - INFO - Saving pre processing data\n",
      "[ 2024-04-01 22:13:04,032 ] 63 root - INFO - Reading dataset as dataframe\n",
      "[ 2024-04-01 22:13:04,032 ] 65 root - INFO - Initiate splitting dataset as train & test set\n",
      "[ 2024-04-01 22:13:04,244 ] 76 root - INFO - Dataset splitted into train and test set\n",
      "[ 2024-04-01 22:13:04,244 ] 77 root - INFO - (27186, 16)\n",
      "[ 2024-04-01 22:13:04,244 ] 78 root - INFO - (9062, 16)\n",
      "[ 2024-04-01 22:13:04,369 ] 94 root - INFO - Reading train and test dataset completed\n",
      "[ 2024-04-01 22:13:04,369 ] 96 root - INFO - Obtaining preprocessor object\n",
      "[ 2024-04-01 22:13:04,766 ] 19 root - INFO - Saving pre processing data\n",
      "[ 2024-04-01 22:13:04,883 ] 107 root - INFO - Dropping target feature from train and test dataframe\n",
      "[ 2024-04-01 22:13:04,898 ] 114 root - INFO - Applying preprocessor object on training and test dataframe\n",
      "[ 2024-04-01 22:13:05,027 ] 119 root - INFO - Successfully applied preprocessor object on train and test data\n",
      "[ 2024-04-01 22:13:05,055 ] 122 root - INFO - Preprocessor saved to file using joblib\n",
      "[ 2024-04-01 22:13:05,069 ] 127 root - INFO - Returning train and test array\n",
      "[ 2024-04-01 22:13:05,072 ] 129 root - INFO - Saving train and test set in numpy file\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    preprocessor = data_transformation.get_data_transformer_object()\n",
    "    train_path,test_path = data_transformation.train_test_split()\n",
    "    train_arr, test_arr = data_transformation.initiate_data_transformation(train_path,test_path)\n",
    "    \n",
    "except Exception as e:\n",
    "    raise CustomException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = \"artifacts/data_transformation/test_arr.npy\"\n",
    "n = np.load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9062, 29)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= n[:,-1]\n",
    "# x = n[:, :-1]\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  1.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         1.        ,  0.29515174, -0.26213611,  0.21512017, -0.84927963,\n",
       "        -0.17833976, -0.44575299, -0.16245093, -0.06209755, -0.08571331,\n",
       "        -0.5110921 , -0.78930757, -1.12526581,  0.        ],\n",
       "       [ 1.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         1.        , -1.62354629, -0.26213611, -0.93452735, -0.84927963,\n",
       "        -0.17833976, -0.99094931, -0.16245093, -0.06209755, -0.08571331,\n",
       "        -0.18351074, -0.78930757, -0.47238301,  1.        ],\n",
       "       [ 1.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        , -1.62354629, -0.26213611, -0.93452735, -0.84927963,\n",
       "         5.60727451, -0.78215072,  6.15570499, -0.06209755,  5.87540757,\n",
       "        -1.09504148,  0.48002787,  0.83338258,  1.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
